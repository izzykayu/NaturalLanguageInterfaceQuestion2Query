{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class generates vocabulary and word2index and index2word on any corpus\n",
    "class Language():\n",
    "    def __init__(self, filename, filter_special_characters, word_count_threshold, special_symbols_list, pretrained_word2vec_path=None, word_embedding_dim=None):\n",
    "        self.contents = self.read_file(filename)\n",
    "        self.contents = self.normalize_file_contents(self.contents, filter_special_characters)\n",
    "        self.vocabulary = self.generate_vocabulary(self.contents, word_count_threshold, special_symbols_list)\n",
    "        self.word2index = self.generate_word2index(self.vocabulary)\n",
    "        self.index2word = self.generate_index2word(self.vocabulary)\n",
    "        self.filter_special_characters = filter_special_characters\n",
    "        self.special_symbols_list = special_symbols_list\n",
    "        if pretrained_word2vec_path is not None and word_embedding_dim is not None:\n",
    "            self.word_embedding_dim = word_embedding_dim\n",
    "            self.word_embeddings = self.initialize_word_vectors(pretrained_word2vec_path)\n",
    "        else:\n",
    "            self.word_embedding_dim = None\n",
    "            self.word_embeddings = None\n",
    "            \n",
    "    # Returns contents of a file as list of sentences\n",
    "    def read_file(self, filename):\n",
    "        _file = open(filename,'r')\n",
    "        contents = []\n",
    "        for line in _file:\n",
    "            contents.append(line)\n",
    "        _file.close()\n",
    "        return contents\n",
    "\n",
    "    # Lowercase, trim, and remove filter_special_characters \n",
    "    def normalize_file_contents(self, contents, filter_special_characters):\n",
    "        normalized_contents = []\n",
    "        for line in contents:\n",
    "            line = line.lower().strip()\n",
    "            line = line.translate(None, filter_special_characters)\n",
    "            normalized_contents.append(line.split())\n",
    "        return normalized_contents\n",
    "\n",
    "    # Returns the vocabulary- words below a threshold are dropped and special symbols are added(SOS, EOS)   \n",
    "    def generate_vocabulary(self, contents, word_count_threshold, special_symbols_list):\n",
    "        vocab = []\n",
    "        for special_symbols in special_symbols_list:\n",
    "            vocab.append(special_symbols)  \n",
    "\n",
    "        counter = Counter()\n",
    "        for line in contents:\n",
    "            counter.update(line)\n",
    "\n",
    "        for word,count in counter.iteritems():\n",
    "            if count > word_count_threshold:\n",
    "                vocab.append(word)\n",
    "        return vocab\n",
    "\n",
    "    # maps word to index\n",
    "    def generate_word2index(self, vocabulary):\n",
    "        word2index = {}\n",
    "        for index, word in enumerate(vocabulary):\n",
    "            word2index[word] = index\n",
    "        return word2index\n",
    "\n",
    "    # maps index to word\n",
    "    def generate_index2word(self, vocabulary):\n",
    "        index2word = {}\n",
    "        for index, word in enumerate(vocabulary):\n",
    "            index2word[index] = word\n",
    "        return index2word\n",
    "    \n",
    "    def initialize_word_vectors(self, pretrained_word2vec_path):\n",
    "        word2vec_model = KeyedVectors.load_word2vec_format(pretrained_word2vec_path, binary=True)\n",
    "        word_vectors = np.random.uniform(-0.1, 0.1, (len(self.vocabulary), self.word_embedding_dim))\n",
    "        for index, word in self.index2word.iteritems():\n",
    "            if word in word2vec_model:\n",
    "                word_vectors[index, :] = word2vec_model[word]\n",
    "        return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourceLanguage = Language(filename= '../training_data/mimic_tr.nl.tem',\n",
    "                          filter_special_characters= string.punctuation.translate(None, '_'),\n",
    "                          word_count_threshold= 1,\n",
    "                          special_symbols_list=['<sos>','<eos>','<unk>'],\n",
    "                          pretrained_word2vec_path='/scratch/at3577/nli/data/wikipedia-pubmed-and-PMC-w2v.bin', \n",
    "                          word_embedding_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# had to add unkown in sql vocab--> Check this\n",
    "targetLanguage = Language(filename= '../training_data/mimic_tr.sql.tem',\n",
    "                          filter_special_characters= '',\n",
    "                          word_count_threshold= 1,\n",
    "                          special_symbols_list= ['<sos>','<eos>','<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This class is used to load the dataset\n",
    "class Nl2SqlDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.dataframe.iloc[idx, 0]\n",
    "        target = self.dataframe.iloc[idx, 1]\n",
    "        \n",
    "        sample = {'nl': source, 'sql': target}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Transformers:\n",
    "# 1. Lowercase, trim, and remove filter_special_characters\n",
    "# 2. add SOS, EOS and UNK to the sentence\n",
    "# 3. converts sentence to index\n",
    "# 4. to tensor\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self, sourceLanguage, targetLanguage):\n",
    "        self.source_language = sourceLanguage\n",
    "        self.target_language = targetLanguage\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        nl, sql = sample['nl'], sample['sql']\n",
    "        \n",
    "        # 1. Lowercase, trim, and remove filter_special_characters\n",
    "        nl  = self.normalize_line(nl, self.source_language.filter_special_characters)\n",
    "        sql = self.normalize_line(sql, self.target_language.filter_special_characters)\n",
    "        \n",
    "        # 2. add SOS, EOS and UNK to the sentence\n",
    "        nl = self.replace_unknown_words(nl, self.source_language, self.source_language.special_symbols_list[2])\n",
    "        sql = self.replace_unknown_words(sql, self.target_language, self.target_language.special_symbols_list[2])\n",
    "        nl = [self.source_language.special_symbols_list[0]] + nl + [self.source_language.special_symbols_list[1]]\n",
    "        sql= [self.target_language.special_symbols_list[0]] + sql + [self.target_language.special_symbols_list[1]]\n",
    "        \n",
    "        # 3. converts sentence to index\n",
    "        nl  = self.sentence2index(nl, self.source_language)\n",
    "        sql = self.sentence2index(sql, self.target_language)\n",
    "        \n",
    "        # 4. to tensor\n",
    "        nl = torch.LongTensor(nl)\n",
    "        sql = torch.LongTensor(sql)\n",
    "        \n",
    "        return {'nl': nl, 'sql': sql}\n",
    "    \n",
    "    def normalize_line(self, line, filter_special_characters):\n",
    "        line = line.lower().strip()\n",
    "        line = line.translate(None, filter_special_characters)\n",
    "        line = line.split()\n",
    "        return line\n",
    "\n",
    "    def replace_unknown_words(self, sentence, language, unknown_symbol):\n",
    "        for idx, word in enumerate(sentence):\n",
    "            if word not in language.vocabulary:\n",
    "                sentence[idx] = unknown_symbol\n",
    "        return sentence\n",
    "    \n",
    "    def sentence2index(self, sentence, language):\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            new_sentence.append(language.word2index[word])\n",
    "        return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_dataset = Nl2SqlDataset('../training_data/mimic_tr.tem.csv', transform=Transformer(sourceLanguage, targetLanguage))\n",
    "dev_dataset = Nl2SqlDataset('../training_data/mimic_tr.tem.csv', transform=Transformer(sourceLanguage, targetLanguage))\n",
    "#test_dataset = Nl2SqlDataset('../training_data/geo_test.tem.csv', transform=Transformer(sourceLanguage, targetLanguage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_language, lstm_hidden_size, lstm_num_layers, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_embeddings = len(source_language.vocabulary)\n",
    "        self.embedding_dim = source_language.word_embedding_dim\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.bilstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                              hidden_size= lstm_hidden_size,\n",
    "                              num_layers= lstm_num_layers,\n",
    "                              bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.initialize_embeddings(source_language.word_embeddings)\n",
    "        \n",
    "    def initialize_embeddings(self, initial_word_embeddings):\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(initial_word_embeddings))\n",
    "    \n",
    "    def forward(self, source_language_sentence):\n",
    "        \n",
    "        embedded  = self.embedding(source_language_sentence)\n",
    "        embedded  = self.dropout(embedded)\n",
    "        \n",
    "        seq_len = len(source_language_sentence)\n",
    "        bilstm_input = embedded.view(seq_len, 1, self.embedding_dim)\n",
    "        \n",
    "        output, (hidden_state, cell_state) = self.bilstm(bilstm_input)\n",
    "        \n",
    "        return output, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, target_language, encoder, target_embedding_dim, lstm_num_layers, dropout_prob):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.num_embeddings = len(target_language.vocabulary)\n",
    "        self.embedding_dim = target_embedding_dim\n",
    "        self.lstm_hidden_size = encoder.lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.bilstm = nn.LSTM(input_size=self.embedding_dim + encoder.lstm_hidden_size*2,\n",
    "                              hidden_size=self.lstm_hidden_size,\n",
    "                              num_layers= lstm_num_layers,\n",
    "                              bidirectional=True)\n",
    "        self.output_layer = nn.Linear(self.lstm_hidden_size*2 + encoder.lstm_hidden_size*2, len(target_language.vocabulary))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, target_language_word, hidden_state, cell_state, encoder_outputs):\n",
    "        \n",
    "        #1 Embedding layer\n",
    "        embedded = self.embedding(target_language_word)\n",
    "        embedded  = self.dropout(embedded)\n",
    "        bilstm_input = embedded.view(1, 1, self.embedding_dim)\n",
    "       \n",
    "        #2 Attention layer\n",
    "        attn_weights = self.attn(hidden_state, encoder_outputs) #(1, 1, seqlen)\n",
    "        context = torch.bmm(attn_weights, torch.transpose(encoder_outputs, 0, 1)) #(1,1,2*encoder_lstm_hidden_size)\n",
    "        \n",
    "        #3 Bilstm\n",
    "        bilstm_input = torch.cat((bilstm_input, context), 2)\n",
    "        bilstm_output, (hidden_state, cell_state) = self.bilstm(bilstm_input, (hidden_state, cell_state))\n",
    "        \n",
    "        #4 Output Layer\n",
    "        output_layer_input = torch.cat((bilstm_output, context), 2)\n",
    "        output_layer_input = torch.squeeze(output_layer_input, 0) #(1, self.lstm_hidden_size*2 + encoder.lstm_hidden_size*2)\n",
    "        output = self.softmax(self.output_layer(output_layer_input)) #(1, len(target_language.vocabulary))\n",
    "        \n",
    "        return output, hidden_state, cell_state \n",
    "    \n",
    "    def attn(self, hidden_state, encoder_outputs):\n",
    "        seqlen = len(encoder_outputs)\n",
    "        attn_energies = Variable(torch.zeros(seqlen))\n",
    "        \n",
    "        for i in range(seqlen):\n",
    "            attn_energies[i] = torch.dot(hidden_state.view(1,-1), encoder_outputs[i])\n",
    "        \n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(source_language_sentence, target_language_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, clip_gradient):\n",
    "    # set training to true for dropout layers\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Get size of input and target sentences\n",
    "    input_length = source_language_sentence.size()[0]\n",
    "    target_length = target_language_sentence.size()[0]\n",
    "    \n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden_state, encoder_cell_state = encoder(source_language_sentence)\n",
    "    # encoder_outputs -> (input_length, 1, 2*encoder_hidden_size)\n",
    "    # encoder_hidden_state -> (2, 1, encoder_hidden_size)\n",
    "\n",
    "    # Prepare decoder input and output\n",
    "    decoder_hidden_state = encoder_hidden_state\n",
    "    decoder_cell_state = Variable(torch.zeros(decoder_hidden_state.shape[0], decoder_hidden_state.shape[1], decoder_hidden_state.shape[2]))\n",
    "    \n",
    "    #only using teacher forcing for now --> Check this\n",
    "    loss = 0\n",
    "    for i in range(target_length-1):\n",
    "        decoder_output, decoder_hidden_state, decoder_cell_state = decoder(target_language_sentence[i].view(1,1), decoder_hidden_state, decoder_cell_state, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_language_sentence[i+1])\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip_gradient)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip_gradient)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset, num_samples, source_language, target_language, encoder, decoder, criterion, verbose=False):\n",
    "    # set training to false for dropout layers\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for n_sample in range(num_samples):\n",
    "        sample = random.choice(dataset)\n",
    "        source_language_sentence, target_language_sentence = Variable(sample['nl']), Variable(sample['sql'])\n",
    "        \n",
    "        # Get size of input and target sentences\n",
    "        input_length = source_language_sentence.size()[0]\n",
    "        target_length = target_language_sentence.size()[0]\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, encoder_hidden_state, encoder_cell_state = encoder(source_language_sentence)\n",
    "        # encoder_outputs -> (input_length, 1, 2*encoder_hidden_size)\n",
    "        # encoder_hidden_state -> (2, 1, encoder_hidden_size)\n",
    "\n",
    "        # Prepare decoder input and output\n",
    "        decoder_hidden_state = encoder_hidden_state\n",
    "        decoder_cell_state = Variable(torch.zeros(decoder_hidden_state.shape[0], decoder_hidden_state.shape[1], decoder_hidden_state.shape[2]))\n",
    "        \n",
    "        loss = 0\n",
    "        decoder_input = target_language_sentence[0].view(1,1)\n",
    "        predicted_sentence = []\n",
    "        for i in range(target_length-1):\n",
    "            decoder_output, decoder_hidden_state, decoder_cell_state = decoder(decoder_input, decoder_hidden_state, decoder_cell_state, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_language_sentence[i+1])\n",
    "            \n",
    "            # Choose top word from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            if ni == target_language.word2index['<eos>']:\n",
    "                break\n",
    "            \n",
    "            # Next input is chosen word\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            predicted_sentence.append(ni)\n",
    "        \n",
    "        total_loss += (loss.data[0] / target_length)\n",
    "        if verbose:\n",
    "            print 'Sample ' + str(n_sample)\n",
    "            print 'Source sentence = ' + str(to_sentence(source_language_sentence.data.numpy(), source_language))\n",
    "            print 'Target sentence = ' + str(to_sentence(target_language_sentence.data.numpy(), target_language))\n",
    "            print 'Predicted sentence = ' + str(to_sentence(predicted_sentence, target_language))\n",
    "            \n",
    "    return total_loss/num_samples\n",
    "\n",
    "def to_sentence(index_list ,language):\n",
    "    sentence = []\n",
    "    for index in index_list:\n",
    "        sentence.append(language.index2word[index])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize models, optimizers, and a loss function (criterion).\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 70\n",
    "clip_gradient = 5.0\n",
    "\n",
    "encoder_lstm_hidden_size = 100\n",
    "encoder_lstm_num_layers=1\n",
    "encoder_dropout_prob = 0.05\n",
    "\n",
    "decoder_target_embedding_dim=100\n",
    "decoder_lstm_num_layers=1\n",
    "decoder_dropout_prob = 0.05\n",
    "\n",
    "encoder = Encoder(source_language= sourceLanguage, \n",
    "                  lstm_hidden_size= encoder_lstm_hidden_size, \n",
    "                  lstm_num_layers= encoder_lstm_num_layers,\n",
    "                  dropout_prob= encoder_dropout_prob)\n",
    "decoder = AttnDecoder(target_language= targetLanguage, \n",
    "                      encoder= encoder, \n",
    "                      target_embedding_dim= decoder_target_embedding_dim, \n",
    "                      lstm_num_layers= decoder_lstm_num_layers,\n",
    "                      dropout_prob= decoder_dropout_prob)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/2.7.12/intel/lib/python2.7/site-packages/ipykernel/__main__.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/share/apps/python/2.7.12/intel/lib/python2.7/site-packages/ipykernel/__main__.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epoch is 2.24132088039 dev loss= 3.75725344034\n",
      "Training loss after 1 epoch is 0.626169562493 dev loss= 2.36812647828\n",
      "Training loss after 2 epoch is 0.350380761856 dev loss= 2.17197119295\n",
      "Training loss after 3 epoch is 0.244380665842 dev loss= 2.43190895258\n",
      "Training loss after 4 epoch is 0.139430442641 dev loss= 3.13085800388\n",
      "Training loss after 5 epoch is 0.120988297409 dev loss= 0.0939236086692\n",
      "Training loss after 6 epoch is 0.0907304931697 dev loss= 0.0681077098176\n",
      "Training loss after 7 epoch is 0.08055973015 dev loss= 0.565917118298\n",
      "Training loss after 8 epoch is 0.0507540254568 dev loss= 0.0185877809504\n",
      "Training loss after 9 epoch is 0.053658393525 dev loss= 0.220232151594\n",
      "Training loss after 10 epoch is 0.0497627610749 dev loss= 0.607401155891\n",
      "Training loss after 11 epoch is 0.0519580220017 dev loss= 0.025421864002\n",
      "Training loss after 12 epoch is 0.0350311793248 dev loss= 0.0411268949335\n",
      "Training loss after 13 epoch is 0.0359319721932 dev loss= 0.0154833261069\n",
      "Training loss after 14 epoch is 0.0205732926524 dev loss= 0.030627540798\n",
      "Training loss after 15 epoch is 0.0300843182227 dev loss= 0.0151186979945\n",
      "Training loss after 16 epoch is 0.0236953668962 dev loss= 0.0100985493196\n",
      "Training loss after 17 epoch is 0.0145166056976 dev loss= 0.0183019293154\n",
      "Training loss after 18 epoch is 0.032872691289 dev loss= 0.170301996474\n",
      "Training loss after 19 epoch is 0.0294261105275 dev loss= 1.00378697777\n",
      "Training loss after 20 epoch is 0.0128813906031 dev loss= 0.00912624157717\n",
      "Training loss after 21 epoch is 0.0102096162839 dev loss= 0.636585744758\n",
      "Training loss after 22 epoch is 0.0135833275566 dev loss= 0.5907923985\n",
      "Training loss after 23 epoch is 0.00815461594186 dev loss= 0.00064174295423\n",
      "Training loss after 24 epoch is 0.0103310621314 dev loss= 0.00044045205004\n",
      "Training loss after 25 epoch is 0.00925409409261 dev loss= 0.00432275067722\n",
      "Training loss after 26 epoch is 0.00718105988028 dev loss= 0.0181328322102\n",
      "Training loss after 27 epoch is 0.0108282829104 dev loss= 0.00266901639643\n",
      "Training loss after 28 epoch is 0.00758200471589 dev loss= 0.0487345893915\n",
      "Training loss after 29 epoch is 0.00434268123558 dev loss= 0.00274777540029\n",
      "Training loss after 30 epoch is 0.00307146479589 dev loss= 0.00048838934892\n",
      "Training loss after 31 epoch is 0.00625636857827 dev loss= 0.000630612790567\n",
      "Training loss after 32 epoch is 0.0017008939084 dev loss= 0.00182798888498\n",
      "Training loss after 33 epoch is 0.00216120199206 dev loss= 0.000788838071292\n",
      "Training loss after 34 epoch is 0.0015796572715 dev loss= 0.000487915712328\n",
      "Training loss after 35 epoch is 0.00350579876522 dev loss= 0.00396842798407\n",
      "Training loss after 36 epoch is 0.00076497743422 dev loss= 0.000369401098287\n",
      "Training loss after 37 epoch is 0.000373499596822 dev loss= 0.000264925001467\n",
      "Training loss after 38 epoch is 0.000232406837571 dev loss= 0.000310399055644\n",
      "Training loss after 39 epoch is 0.000198358893231 dev loss= 0.000231422011964\n",
      "Training loss after 40 epoch is 0.000261216272713 dev loss= 0.000199781815867\n",
      "Training loss after 41 epoch is 0.00019216379222 dev loss= 0.000111573038327\n",
      "Training loss after 42 epoch is 0.000187973449017 dev loss= 0.000124043806916\n",
      "Training loss after 43 epoch is 0.000159022917861 dev loss= 0.00011639146632\n",
      "Training loss after 44 epoch is 0.000140805945611 dev loss= 0.000146580982953\n",
      "Training loss after 45 epoch is 0.000131166553633 dev loss= 7.41067123023e-05\n",
      "Training loss after 46 epoch is 0.000144817915343 dev loss= 7.52703045345e-05\n",
      "Training loss after 47 epoch is 0.000125084553119 dev loss= 7.77992171287e-05\n",
      "Training loss after 48 epoch is 9.93705319917e-05 dev loss= 8.64159891712e-05\n",
      "Training loss after 49 epoch is 9.22490377413e-05 dev loss= 6.06778059812e-05\n",
      "Training loss after 50 epoch is 0.0001118625862 dev loss= 7.52028551905e-05\n",
      "Training loss after 51 epoch is 6.99247877058e-05 dev loss= 7.48509442311e-05\n",
      "Training loss after 52 epoch is 8.91002544931e-05 dev loss= 0.000133112109125\n",
      "Training loss after 53 epoch is 7.35191604756e-05 dev loss= 6.96869316858e-05\n",
      "Training loss after 54 epoch is 7.0728681991e-05 dev loss= 6.30628769072e-05\n",
      "Training loss after 55 epoch is 7.21250145815e-05 dev loss= 4.0885813701e-05\n",
      "Training loss after 56 epoch is 5.66850560932e-05 dev loss= 5.08134408018e-05\n",
      "Training loss after 57 epoch is 6.38724714044e-05 dev loss= 5.26478924485e-05\n",
      "Training loss after 58 epoch is 5.00787073293e-05 dev loss= 3.69034398154e-05\n",
      "Training loss after 59 epoch is 5.90770374108e-05 dev loss= 4.42047461059e-05\n",
      "Training loss after 60 epoch is 5.06177179061e-05 dev loss= 3.95405488567e-05\n",
      "Training loss after 61 epoch is 4.68763828976e-05 dev loss= 3.91841814379e-05\n",
      "Training loss after 62 epoch is 3.91278549826e-05 dev loss= 2.93800621668e-05\n",
      "Training loss after 63 epoch is 4.132640103e-05 dev loss= 2.6096619708e-05\n",
      "Training loss after 64 epoch is 3.95777205754e-05 dev loss= 3.97755116006e-05\n",
      "Training loss after 65 epoch is 3.42492723992e-05 dev loss= 3.29960084731e-05\n",
      "Training loss after 66 epoch is 2.96200940603e-05 dev loss= 3.34455886891e-05\n",
      "Training loss after 67 epoch is 3.27019236349e-05 dev loss= 1.98775956319e-05\n",
      "Training loss after 68 epoch is 3.10369140643e-05 dev loss= 2.34726579484e-05\n",
      "Training loss after 69 epoch is 3.28738597107e-05 dev loss= 2.31025610883e-05\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for n_epochs in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    for batch in range(batch_size):\n",
    "        sample = random.choice(training_dataset)\n",
    "        source_language_sentence, target_language_sentence = Variable(sample['nl']), Variable(sample['sql'])\n",
    "        training_loss += train(source_language_sentence, target_language_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, clip_gradient)\n",
    "    \n",
    "    training_loss = training_loss/batch_size\n",
    "    dev_loss = evaluate(dev_dataset, 10, sourceLanguage, targetLanguage, encoder, decoder, criterion)\n",
    "    print 'Training loss after ' + str(n_epochs) + ' epoch is ' + str(training_loss) + ' dev loss= ' + str(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/2.7.12/intel/lib/python2.7/site-packages/ipykernel/__main__.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/share/apps/python/2.7.12/intel/lib/python2.7/site-packages/ipykernel/__main__.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Source sentence = ['<sos>', 'how', 'many', 'problem_1', 'patients', 'have', 'been', 'tested', 'for', 'test_1', 'below', 'numeric_1', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%')\", 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '<', 'numeric_1', ')', '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%')\", 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '<', 'numeric_1', ')']\n",
      "Sample 1\n",
      "Source sentence = ['<sos>', 'how', 'many', 'patients', 'had', 'labs', 'for', 'test_1', 'whose', 'results', 'were', 'between', 'numeric_1', 'and', 'numeric_2', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '>', 'numeric_1', 'and', 'valuenum', '<', 'numeric_2', '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '>', 'numeric_1', 'and', 'valuenum', '<', 'numeric_2']\n",
      "Sample 2\n",
      "Source sentence = ['<sos>', 'how', 'many', 'problem_1', 'patients', 'have', 'been', 'given', 'test_1', 'test', 'whose', 'results', 'were', 'greater', 'than', 'numeric_1', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%')\", 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '>', 'numeric_1', ')', '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%')\", 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '>', 'numeric_1', ')']\n",
      "Sample 3\n",
      "Source sentence = ['<sos>', 'how', 'many', 'patients', 'had', 'test_1', 'test', 'performed', 'and', 'whose', 'results', 'were', 'between', 'numeric_1', 'and', 'numeric_2', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '>', 'numeric_1', 'and', 'valuenum', '<', 'numeric_2', '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'labevents', 'where', 'itemid', 'in', '(', 'select', 'distinct', 'itemid', 'from', 'd_labitems', 'where', 'label', 'like', \"'%test_1%')\", 'and', 'valuenum', '>', 'numeric_1', 'and', 'valuenum', '<', 'numeric_2']\n",
      "Sample 4\n",
      "Source sentence = ['<sos>', 'how', 'many', 'problem_1', 'patients', 'were', 'over', 'numeric_1', 'years', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'admissions.subject_id)', 'from', 'admissions,', 'patients', 'where', 'admissions.subject_id', 'in(select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'))\", 'and', 'patients.subject_id', '=', 'admissions.subject_id', 'and', 'round(', '(cast(admittime', 'as', 'date)', '-', 'cast(dob', 'as', 'date))', '/', '365.242)', '>', 'numeric_1', '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'admissions.subject_id)', 'from', 'admissions,', 'patients', 'where', 'admissions.subject_id', 'in(select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'))\", 'and', 'patients.subject_id', '=', 'admissions.subject_id', 'and', 'round(', '(cast(admittime', 'as', 'date)', '-', 'cast(dob', 'as', 'date))', '/', '365.242)', '>', 'numeric_1']\n",
      "Sample 5\n",
      "Source sentence = ['<sos>', 'which', 'patients', 'died', 'with', 'problem_1', 'in', 'the', 'hospital', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'distinct', 'subject_id', 'from', 'patients', 'where', 'dod_hosp', 'is', 'not', 'null', 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'))\", '<eos>']\n",
      "Predicted sentence = ['select', 'distinct', 'subject_id', 'from', 'patients', 'where', 'dod_hosp', 'is', 'not', 'null', 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'))\"]\n",
      "Sample 6\n",
      "Source sentence = ['<sos>', 'how', 'many', 'males', 'suffer', 'from', 'problem_1', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'patients', 'where', 'subject_id', 'in', '(', 'select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'\", '))', 'and', 'gender', '=', \"'m'\", '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'patients', 'where', 'subject_id', 'in', '(', 'select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'\", '))', 'and', 'gender', '=', \"'m'\"]\n",
      "Sample 7\n",
      "Source sentence = ['<sos>', 'which', 'problem_1', 'patients', 'expired', 'in', 'the', 'hospital', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'distinct', 'subject_id', 'from', 'patients', 'where', 'dod_hosp', 'is', 'not', 'null', 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'))\", '<eos>']\n",
      "Predicted sentence = ['select', 'distinct', 'subject_id', 'from', 'patients', 'where', 'dod_hosp', 'is', 'not', 'null', 'and', 'subject_id', 'in', '(select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'))\"]\n",
      "Sample 8\n",
      "Source sentence = ['<sos>', 'how', 'many', 'patients', 'suffer', 'from', 'problem_1', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%')\", '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%')\"]\n",
      "Sample 9\n",
      "Source sentence = ['<sos>', 'how', 'many', 'patients', 'have', 'problem_1', 'and', 'are', 'females', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count(distinct', 'subject_id)', 'from', 'patients', 'where', 'subject_id', 'in', '(', 'select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'\", '))', 'and', 'gender', '=', \"'f'\", '<eos>']\n",
      "Predicted sentence = ['select', 'count(distinct', 'subject_id)', 'from', 'patients', 'where', 'subject_id', 'in', '(', 'select', 'distinct', 'subject_id', 'from', 'diagnoses_icd', 'where', 'icd9_code', 'in', '(select', 'distinct', 'icd9_code', 'from', 'd_icd_diagnoses', 'where', 'long_title', 'like', \"'%problem_1%'\", '))', 'and', 'gender', '=', \"'f'\"]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on small sample of test set\n",
    "test_loss = evaluate(training_dataset, 10, sourceLanguage, targetLanguage, encoder, decoder, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9266274811125673e-05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To start/stop the database server\n",
    "#pg_ctl -D /scratch/at3577/postgresql/data -l /scratch/at3577/postgresql/logfile start\n",
    "#pg_ctl stop -D /scratch/at3577/postgresql/data -m smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id|subject_id|hadm_id|     admittime     |     dischtime     |     deathtime     |admission_type|   admission_location    |   discharge_location    |insurance|language|    religion     |marital_status|      ethnicity      |     edregtime     |     edouttime     |                       diagnosis                        |hospital_expire_flag|has_chartevents_data\n",
      "------+----------+-------+-------------------+-------------------+-------------------+--------------+-------------------------+-------------------------+---------+--------+-----------------+--------------+---------------------+-------------------+-------------------+--------------------------------------------------------+--------------------+--------------------\n",
      "    21|        22| 165315|2196-04-09 12:26:00|2196-04-10 15:54:00|                   |EMERGENCY     |EMERGENCY ROOM ADMIT     |DISC-TRAN CANCER/CHLDRN H|Private  |        |UNOBTAINABLE     |MARRIED       |WHITE                |2196-04-09 10:06:00|2196-04-09 13:24:00|BENZODIAZEPINE OVERDOSE                                 |                   0|                   1\n",
      "    22|        23| 152223|2153-09-03 07:15:00|2153-09-08 19:10:00|                   |ELECTIVE      |PHYS REFERRAL/NORMAL DELI|HOME HEALTH CARE         |Medicare |        |CATHOLIC         |MARRIED       |WHITE                |                   |                   |CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS GRAFT/SDA|                   0|                   1\n",
      "    23|        23| 124321|2157-10-18 19:34:00|2157-10-25 14:00:00|                   |EMERGENCY     |TRANSFER FROM HOSP/EXTRAM|HOME HEALTH CARE         |Medicare |ENGL    |CATHOLIC         |MARRIED       |WHITE                |                   |                   |BRAIN MASS                                              |                   0|                   1\n",
      "    24|        24| 161859|2139-06-06 16:14:00|2139-06-09 12:48:00|                   |EMERGENCY     |TRANSFER FROM HOSP/EXTRAM|HOME                     |Private  |        |PROTESTANT QUAKER|SINGLE        |WHITE                |                   |                   |INTERIOR MYOCARDIAL INFARCTION                          |                   0|                   1\n",
      "    25|        25| 129635|2160-11-02 02:06:00|2160-11-05 14:55:00|                   |EMERGENCY     |EMERGENCY ROOM ADMIT     |HOME                     |Private  |        |UNOBTAINABLE     |MARRIED       |WHITE                |2160-11-02 01:01:00|2160-11-02 04:27:00|ACUTE CORONARY SYNDROME                                 |                   0|                   1\n",
      "    26|        26| 197661|2126-05-06 15:16:00|2126-05-13 15:00:00|                   |EMERGENCY     |TRANSFER FROM HOSP/EXTRAM|HOME                     |Medicare |        |CATHOLIC         |SINGLE        |UNKNOWN/NOT SPECIFIED|                   |                   |V-TACH                                                  |                   0|                   1\n",
      "    27|        27| 134931|2191-11-30 22:16:00|2191-12-03 14:45:00|                   |NEWBORN       |PHYS REFERRAL/NORMAL DELI|HOME                     |Private  |        |CATHOLIC         |              |WHITE                |                   |                   |NEWBORN                                                 |                   0|                   1\n",
      "    28|        28| 162569|2177-09-01 07:15:00|2177-09-06 16:00:00|                   |ELECTIVE      |PHYS REFERRAL/NORMAL DELI|HOME HEALTH CARE         |Medicare |        |CATHOLIC         |MARRIED       |WHITE                |                   |                   |CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS GRAFT/SDA|                   0|                   1\n",
      "    29|        30| 104557|2172-10-14 14:17:00|2172-10-19 14:37:00|                   |URGENT        |TRANSFER FROM HOSP/EXTRAM|HOME HEALTH CARE         |Medicare |        |CATHOLIC         |MARRIED       |UNKNOWN/NOT SPECIFIED|                   |                   |UNSTABLE ANGINA\\CATH                                    |                   0|                   1\n",
      "    30|        31| 128652|2108-08-22 23:27:00|2108-08-30 15:00:00|2108-08-30 15:00:00|EMERGENCY     |TRANSFER FROM HOSP/EXTRAM|DEAD/EXPIRED             |Medicare |        |CATHOLIC         |MARRIED       |WHITE                |                   |                   |STATUS EPILEPTICUS                                      |                   1|                   1\n",
      "(10 rows)\n"
     ]
    }
   ],
   "source": [
    "from pg import DB\n",
    "db = DB( host='localhost', user='postgres', dbname='mimic' )\n",
    "print(db.query('select * from mimiciii.admissions limit 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
