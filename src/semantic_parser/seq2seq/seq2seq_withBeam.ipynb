{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class generates vocabulary and word2index and index2word on any corpus\n",
    "class Language():\n",
    "    def __init__(self, filename, filter_special_characters, word_count_threshold, special_symbols_list, pretrained_word2vec_path=None, word_embedding_dim=None):\n",
    "        self.contents = self.read_file(filename)\n",
    "        self.contents = self.normalize_file_contents(self.contents, filter_special_characters)\n",
    "        self.vocabulary = self.generate_vocabulary(self.contents, word_count_threshold, special_symbols_list)\n",
    "        self.word2index = self.generate_word2index(self.vocabulary)\n",
    "        self.index2word = self.generate_index2word(self.vocabulary)\n",
    "        self.filter_special_characters = filter_special_characters\n",
    "        self.special_symbols_list = special_symbols_list\n",
    "        if pretrained_word2vec_path is not None and word_embedding_dim is not None:\n",
    "            self.word_embedding_dim = word_embedding_dim\n",
    "            self.word_embeddings = self.initialize_word_vectors(pretrained_word2vec_path)\n",
    "        else:\n",
    "            self.word_embedding_dim = None\n",
    "            self.word_embeddings = None\n",
    "            \n",
    "    # Returns contents of a file as list of sentences\n",
    "    def read_file(self, filename):\n",
    "        _file = open(filename,'r')\n",
    "        contents = []\n",
    "        for line in _file:\n",
    "            contents.append(line)\n",
    "        _file.close()\n",
    "        return contents\n",
    "\n",
    "    # Lowercase, trim, and remove filter_special_characters \n",
    "    def normalize_file_contents(self, contents, filter_special_characters):\n",
    "        normalized_contents = []\n",
    "        for line in contents:\n",
    "            line = line.lower().strip()\n",
    "            line = line.translate(None, filter_special_characters)\n",
    "            normalized_contents.append(line.split())\n",
    "        return normalized_contents\n",
    "\n",
    "    # Returns the vocabulary- words below a threshold are dropped and special symbols are added(SOS, EOS)   \n",
    "    def generate_vocabulary(self, contents, word_count_threshold, special_symbols_list):\n",
    "        vocab = []\n",
    "        for special_symbols in special_symbols_list:\n",
    "            vocab.append(special_symbols)  \n",
    "\n",
    "        counter = Counter()\n",
    "        for line in contents:\n",
    "            counter.update(line)\n",
    "\n",
    "        for word,count in counter.iteritems():\n",
    "            if count > word_count_threshold:\n",
    "                vocab.append(word)\n",
    "        return vocab\n",
    "\n",
    "    # maps word to index\n",
    "    def generate_word2index(self, vocabulary):\n",
    "        word2index = {}\n",
    "        for index, word in enumerate(vocabulary):\n",
    "            word2index[word] = index\n",
    "        return word2index\n",
    "\n",
    "    # maps index to word\n",
    "    def generate_index2word(self, vocabulary):\n",
    "        index2word = {}\n",
    "        for index, word in enumerate(vocabulary):\n",
    "            index2word[index] = word\n",
    "        return index2word\n",
    "    \n",
    "    def initialize_word_vectors(self, pretrained_word2vec_path):\n",
    "        word2vec_model = KeyedVectors.load_word2vec_format(pretrained_word2vec_path, binary=True)\n",
    "        word_vectors = np.random.uniform(-0.1, 0.1, (len(self.vocabulary), self.word_embedding_dim))\n",
    "        for index, word in self.index2word.iteritems():\n",
    "            if word in word2vec_model:\n",
    "                word_vectors[index, :] = word2vec_model[word]\n",
    "        return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceLanguage = Language(filename= '../training_data/geo_tr.nl.tem',\n",
    "                          filter_special_characters= string.punctuation.translate(None, '@'),\n",
    "                          word_count_threshold= 1,\n",
    "                          special_symbols_list=['<sos>','<eos>','<unk>'],\n",
    "                          pretrained_word2vec_path='/Users/wyz0214/Downloads/GoogleNews-vectors-negative300.bin', \n",
    "                          word_embedding_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to add unkown in sql vocab--> Check this\n",
    "targetLanguage = Language(filename= '../training_data/geo_tr.sql.tem',\n",
    "                          filter_special_characters= '',\n",
    "                          word_count_threshold= 1,\n",
    "                          special_symbols_list= ['<sos>','<eos>','<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class is used to load the dataset\n",
    "class Nl2SqlDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.dataframe.iloc[idx, 0]\n",
    "        target = self.dataframe.iloc[idx, 1]\n",
    "        \n",
    "        sample = {'nl': source, 'sql': target}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Transformers:\n",
    "# 1. Lowercase, trim, and remove filter_special_characters\n",
    "# 2. add SOS, EOS and UNK to the sentence\n",
    "# 3. converts sentence to index\n",
    "# 4. to tensor\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self, sourceLanguage, targetLanguage):\n",
    "        self.source_language = sourceLanguage\n",
    "        self.target_language = targetLanguage\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        nl, sql = sample['nl'], sample['sql']\n",
    "        \n",
    "        # 1. Lowercase, trim, and remove filter_special_characters\n",
    "        nl  = self.normalize_line(nl, self.source_language.filter_special_characters)\n",
    "        sql = self.normalize_line(sql, self.target_language.filter_special_characters)\n",
    "        \n",
    "        # 2. add SOS, EOS and UNK to the sentence\n",
    "        nl = self.replace_unknown_words(nl, self.source_language, self.source_language.special_symbols_list[2])\n",
    "        sql = self.replace_unknown_words(sql, self.target_language, self.target_language.special_symbols_list[2])\n",
    "        nl = [self.source_language.special_symbols_list[0]] + nl + [self.source_language.special_symbols_list[1]]\n",
    "        sql= [self.target_language.special_symbols_list[0]] + sql + [self.target_language.special_symbols_list[1]]\n",
    "        \n",
    "        # 3. converts sentence to index\n",
    "        nl  = self.sentence2index(nl, self.source_language)\n",
    "        sql = self.sentence2index(sql, self.target_language)\n",
    "        \n",
    "        # 4. to tensor\n",
    "        nl = torch.LongTensor(nl)\n",
    "        sql = torch.LongTensor(sql)\n",
    "        \n",
    "        return {'nl': nl, 'sql': sql}\n",
    "    \n",
    "    def normalize_line(self, line, filter_special_characters):\n",
    "        line = line.lower().strip()\n",
    "        line = line.translate(None, filter_special_characters)\n",
    "        line = line.split()\n",
    "        return line\n",
    "\n",
    "    def replace_unknown_words(self, sentence, language, unknown_symbol):\n",
    "        for idx, word in enumerate(sentence):\n",
    "            if word not in language.vocabulary:\n",
    "                sentence[idx] = unknown_symbol\n",
    "        return sentence\n",
    "    \n",
    "    def sentence2index(self, sentence, language):\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            new_sentence.append(language.word2index[word])\n",
    "        return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_dataset = Nl2SqlDataset('../training_data/geo_train.tem.csv', transform=Transformer(sourceLanguage, targetLanguage))\n",
    "dev_dataset = Nl2SqlDataset('../training_data/geo_dev.tem.csv', transform=Transformer(sourceLanguage, targetLanguage))\n",
    "test_dataset = Nl2SqlDataset('../training_data/geo_test.tem.csv', transform=Transformer(sourceLanguage, targetLanguage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_language, lstm_hidden_size, lstm_num_layers, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_embeddings = len(source_language.vocabulary)\n",
    "        self.embedding_dim = source_language.word_embedding_dim\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.bilstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                              hidden_size= lstm_hidden_size,\n",
    "                              num_layers= lstm_num_layers,\n",
    "                              bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.initialize_embeddings(source_language.word_embeddings)\n",
    "        \n",
    "    def initialize_embeddings(self, initial_word_embeddings):\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(initial_word_embeddings))\n",
    "    \n",
    "    def forward(self, source_language_sentence):\n",
    "        \n",
    "        embedded  = self.embedding(source_language_sentence)\n",
    "        embedded  = self.dropout(embedded)\n",
    "        \n",
    "        seq_len = len(source_language_sentence)\n",
    "        bilstm_input = embedded.view(seq_len, 1, self.embedding_dim)\n",
    "        \n",
    "        output, (hidden_state, cell_state) = self.bilstm(bilstm_input)\n",
    "        \n",
    "        return output, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, target_language, encoder, target_embedding_dim, lstm_num_layers, dropout_prob):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.num_embeddings = len(target_language.vocabulary)\n",
    "        self.embedding_dim = target_embedding_dim\n",
    "        self.lstm_hidden_size = encoder.lstm_hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.bilstm = nn.LSTM(input_size=self.embedding_dim + encoder.lstm_hidden_size*2,\n",
    "                              hidden_size=self.lstm_hidden_size,\n",
    "                              num_layers= lstm_num_layers,\n",
    "                              bidirectional=True)\n",
    "        self.output_layer = nn.Linear(self.lstm_hidden_size*2 + encoder.lstm_hidden_size*2, len(target_language.vocabulary))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, target_language_word, hidden_state, cell_state, encoder_outputs):\n",
    "        \n",
    "        #1 Embedding layer\n",
    "        embedded = self.embedding(target_language_word)\n",
    "        embedded  = self.dropout(embedded)\n",
    "        bilstm_input = embedded.view(1, 1, self.embedding_dim)\n",
    "       \n",
    "        #2 Attention layer\n",
    "        attn_weights = self.attn(hidden_state, encoder_outputs) #(1, 1, seqlen)\n",
    "        context = torch.bmm(attn_weights, torch.transpose(encoder_outputs, 0, 1)) #(1,1,2*encoder_lstm_hidden_size)\n",
    "        \n",
    "        #3 Bilstm\n",
    "        bilstm_input = torch.cat((bilstm_input, context), 2)\n",
    "        bilstm_output, (hidden_state, cell_state) = self.bilstm(bilstm_input, (hidden_state, cell_state))\n",
    "        \n",
    "        #4 Output Layer\n",
    "        output_layer_input = torch.cat((bilstm_output, context), 2)\n",
    "        output_layer_input = torch.squeeze(output_layer_input, 0) #(1, self.lstm_hidden_size*2 + encoder.lstm_hidden_size*2)\n",
    "        output = self.softmax(self.output_layer(output_layer_input)) #(1, len(target_language.vocabulary))\n",
    "        \n",
    "        return output, hidden_state, cell_state \n",
    "    \n",
    "    def attn(self, hidden_state, encoder_outputs):\n",
    "        seqlen = len(encoder_outputs)\n",
    "        attn_energies = Variable(torch.zeros(seqlen))\n",
    "        \n",
    "        for i in range(seqlen):\n",
    "            attn_energies[i] = torch.dot(hidden_state.view(1,-1), encoder_outputs[i])\n",
    "        \n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(source_language_sentence, target_language_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, clip_gradient):\n",
    "    # set training to true for dropout layers\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Get size of input and target sentences\n",
    "    input_length = source_language_sentence.size()[0]\n",
    "    target_length = target_language_sentence.size()[0]\n",
    "    \n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden_state, encoder_cell_state = encoder(source_language_sentence)\n",
    "    # encoder_outputs -> (input_length, 1, 2*encoder_hidden_size)\n",
    "    # encoder_hidden_state -> (2, 1, encoder_hidden_size)\n",
    "\n",
    "    # Prepare decoder input and output\n",
    "    decoder_hidden_state = encoder_hidden_state\n",
    "    decoder_cell_state = Variable(torch.zeros(decoder_hidden_state.shape[0], decoder_hidden_state.shape[1], decoder_hidden_state.shape[2]))\n",
    "    \n",
    "    #only using teacher forcing for now --> Check this\n",
    "    loss = 0\n",
    "    for i in range(target_length-1):\n",
    "        decoder_output, decoder_hidden_state, decoder_cell_state = decoder(target_language_sentence[i].view(1,1), decoder_hidden_state, decoder_cell_state, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_language_sentence[i+1])\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip_gradient)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip_gradient)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset, num_samples, source_language, target_language, encoder, decoder, criterion, verbose=False):\n",
    "    # set training to false for dropout layers\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for n_sample in range(num_samples):\n",
    "        sample = random.choice(dataset)\n",
    "        source_language_sentence, target_language_sentence = Variable(sample['nl']), Variable(sample['sql'])\n",
    "        \n",
    "        # Get size of input and target sentences\n",
    "        input_length = source_language_sentence.size()[0]\n",
    "        target_length = target_language_sentence.size()[0]\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, encoder_hidden_state, encoder_cell_state = encoder(source_language_sentence)\n",
    "        # encoder_outputs -> (input_length, 1, 2*encoder_hidden_size)\n",
    "        # encoder_hidden_state -> (2, 1, encoder_hidden_size)\n",
    "\n",
    "        # Prepare decoder input and output\n",
    "        decoder_hidden_state = encoder_hidden_state\n",
    "        decoder_cell_state = Variable(torch.zeros(decoder_hidden_state.shape[0], decoder_hidden_state.shape[1], decoder_hidden_state.shape[2]))\n",
    "        \n",
    "        loss = 0\n",
    "        decoder_input = target_language_sentence[0].view(1,1)\n",
    "        print(decoder_input)\n",
    "        predicted_sentence = []\n",
    "        for i in range(target_length-1):\n",
    "            decoder_output, decoder_hidden_state, decoder_cell_state = decoder(decoder_input, decoder_hidden_state, decoder_cell_state, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_language_sentence[i+1])\n",
    "            \n",
    "            # Choose top word from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            if ni == target_language.word2index['<eos>']:\n",
    "                #print('!!!!!')\n",
    "                break\n",
    "            \n",
    "            # Next input is chosen word\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            predicted_sentence.append(ni)\n",
    "        \n",
    "        total_loss += (loss.data[0] / target_length)\n",
    "        if verbose:\n",
    "            print 'Sample ' + str(n_sample)\n",
    "            print 'Source sentence = ' + str(to_sentence(source_language_sentence.data.numpy(), source_language))\n",
    "            print 'Target sentence = ' + str(to_sentence(target_language_sentence.data.numpy(), target_language))\n",
    "            print 'Predicted sentence = ' + str(to_sentence(predicted_sentence, target_language))\n",
    "            \n",
    "    return total_loss/num_samples\n",
    "\n",
    "def to_sentence(index_list ,language):\n",
    "    sentence = []\n",
    "    for index in index_list:\n",
    "        sentence.append(language.index2word[index])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_withBeam(dataset, num_samples, source_language, target_language, encoder, decoder, criterion, num_beam=1, verbose=False):\n",
    "    # set training to false for dropout layers\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for n_sample in range(num_samples):\n",
    "        sample = random.choice(dataset)\n",
    "        source_language_sentence, target_language_sentence = Variable(sample['nl']), Variable(sample['sql'])\n",
    "        \n",
    "        # Get size of input and target sentences\n",
    "        input_length = source_language_sentence.size()[0]\n",
    "        target_length = target_language_sentence.size()[0]\n",
    "\n",
    "        # Run words through encoder\n",
    "        encoder_outputs, encoder_hidden_state, encoder_cell_state = encoder(source_language_sentence)\n",
    "        # encoder_outputs -> (input_length, 1, 2*encoder_hidden_size)\n",
    "        # encoder_hidden_state -> (2, 1, encoder_hidden_size)\n",
    "\n",
    "        # Prepare decoder input and output\n",
    "        decoder_hidden_state = encoder_hidden_state\n",
    "        decoder_cell_state = Variable(torch.zeros(decoder_hidden_state.shape[0], decoder_hidden_state.shape[1], decoder_hidden_state.shape[2]))\n",
    "        \n",
    "        decoder_input = target_language_sentence[0].view(1,1)\n",
    "        \n",
    "        \n",
    "        loss = []\n",
    "        predicted_sentences = []\n",
    "        \n",
    "        decoder_output, decoder_hidden_state, decoder_cell_state = decoder(decoder_input, decoder_hidden_state, decoder_cell_state, encoder_outputs)\n",
    "            \n",
    "        topv, topi = decoder_output.data.topk(num_beam)\n",
    "        for m in range(num_beam):\n",
    "            predicted_sentences.append([[topi[0][m]], -topv[0][m]])\n",
    "            loss.append(criterion(decoder_output, target_language_sentence[1]))\n",
    "        #print(predicted_sentences)\n",
    "        \n",
    "        for i in range(target_length-2): #target_length-1\n",
    "            temp_loss = []\n",
    "            temp_sentences = []\n",
    "            for ct, n in enumerate(predicted_sentences):\n",
    "                if n[0][-1] == '<eos>':\n",
    "                    temp_sentences.append(n)\n",
    "                    temp_loss.append(loss[ct])\n",
    "                    continue\n",
    "                    \n",
    "                decoder_input = Variable(torch.LongTensor([[n[0][-1]]]))\n",
    "                decoder_output, decoder_hidden_state, decoder_cell_state = decoder(decoder_input, decoder_hidden_state, decoder_cell_state, encoder_outputs)\n",
    "\n",
    "                #print(decoder_output.data)\n",
    "                #print(topi)\n",
    "\n",
    "                topv, topi = decoder_output.data.topk(num_beam)\n",
    "                for j in range(num_beam):\n",
    "                    ni = topi[0][j]\n",
    "                    wordLst = n[0][:]\n",
    "                    temp_loss.append(loss[ct]+criterion(decoder_output, target_language_sentence[i+1]))\n",
    "                    #print(ni)\n",
    "                    if ni == target_language.word2index['<eos>']:\n",
    "                        print(wordLst)\n",
    "                        wordLst.append('<eos>')\n",
    "                        temp_sentences.append([wordLst, float('Inf')])\n",
    "                    else:\n",
    "                        wordLst.append(ni)\n",
    "                        temp_sentences.append([wordLst, n[1]-topv[0][j]])\n",
    "            #print(temp_sentences)\n",
    "            predicted_sentences = sorted(temp_sentences, key=lambda lst: lst[1])[:num_beam][:]\n",
    "            print(predicted_sentences)\n",
    "            predicted_ind = sorted(range(len(temp_sentences)), key=lambda i: temp_sentences[i])\n",
    "            loss = [x for _, x in sorted(zip(predicted_ind, temp_loss), key=lambda pair: pair[0])][:num_beam]\n",
    "            \n",
    "        #print(predicted_sentences)\n",
    "        predicted_sentence = sorted(predicted_sentences, key=lambda lst: lst[1])[0][0]\n",
    "        if predicted_sentence[-1] == '<eos>':\n",
    "            predicted_sentence = predicted_sentence[:-1]\n",
    "        predicted_i = sorted(range(len(predicted_sentences)), key=lambda i: predicted_sentences[i])\n",
    "        loss_ = [x for _, x in sorted(zip(predicted_i, loss), key=lambda pair: pair[0])][0]\n",
    "            \n",
    "        total_loss += (loss_.data[0] / target_length)\n",
    "        if verbose:\n",
    "            print 'Sample ' + str(n_sample)\n",
    "            print 'Source sentence = ' + str(to_sentence(source_language_sentence.data.numpy(), source_language))\n",
    "            print 'Target sentence = ' + str(to_sentence(target_language_sentence.data.numpy(), target_language))\n",
    "            print 'Predicted sentence = ' + str(to_sentence(predicted_sentence, target_language))\n",
    "            \n",
    "    return total_loss/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[56, 54], 0.3299456719250884]]\n",
      "[[[56, 54, 58], 0.3299702196381986]]\n",
      "[[[56, 54, 58, 69], 0.33041607023915276]]\n",
      "[[[56, 54, 58, 69, 46], 0.33073886652709916]]\n",
      "[[[56, 54, 58, 69, 46, 64], 0.3328757959534414]]\n",
      "[[[56, 54, 58, 69, 46, 64, 84], 0.33350016607437283]]\n",
      "[[[56, 54, 58, 69, 46, 64, 84, 5], 0.3574521156260744]]\n",
      "[[[56, 54, 58, 69, 46, 64, 84, 5, 36], 0.36073526937980205]]\n",
      "[56, 54, 58, 69, 46, 64, 84, 5, 36]\n",
      "[[[56, 54, 58, 69, 46, 64, 84, 5, 36, '<eos>'], inf]]\n",
      "Sample 0\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'lowest', 'point', 'in', 'state@0', 'in', '<unk>', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'highlow.lowest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'highlow.lowest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';']\n",
      "[[[56, 35], 0.38689592983791954]]\n",
      "[[[56, 35, 58], 0.38924827673690743]]\n",
      "[[[56, 35, 58, 33], 0.38943239506261307]]\n",
      "[[[56, 35, 58, 33, 46], 0.42994335498588043]]\n",
      "[[[56, 35, 58, 33, 46, 78], 1.2554668506600137]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84], 1.306677447817492]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23], 2.2679111829020258]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72], 2.2794358815990563]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16], 2.3008164271705027]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36], 2.5181000336997386]]\n",
      "[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "[[[56, 35, 58, 33, 46, 78, 84, 23, 72, 16, 36, '<eos>'], inf]]\n",
      "Sample 1\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'capital', 'of', 'the', 'smallest', 'state', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'state.capital', 'from', 'state', 'where', 'state.area', '=', '(', 'select', 'min', '(', 'state.area', ')', 'from', 'state', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'state.capital', 'from', 'state', 'where', 'state.state_name', '=', \"'district\", 'of', \"columbia'\", ';']\n",
      "[[[56, 12], 0.08364703499682946]]\n",
      "[[[56, 12, 58], 0.08394468473125016]]\n",
      "[[[56, 12, 58, 22], 0.08423411371040856]]\n",
      "[[[56, 12, 58, 22, 46], 0.1162295405738405]]\n",
      "[[[56, 12, 58, 22, 46, 12], 0.8594339554183534]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84], 0.8695466415010742]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26], 0.8914593485169462]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36], 1.0138203901224188]]\n",
      "[56, 12, 58, 22, 46, 12, 84, 26, 36]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "Sample 2\n",
      "Source sentence = ['<sos>', 'what', 'river', 'flows', 'through', 'the', 'most', 'states', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'river.river_name', 'from', 'river', 'group', 'by', '(', 'river.river_name', ')', 'order', 'by', 'count', '(', 'distinct', 'river.traverse', ')', 'desc', 'limit', '1', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.river_name', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ';']\n",
      "[[[56, 48], 0.8035351478174562]]\n",
      "[[[56, 48, 58], 0.8053116322407732]]\n",
      "[[[56, 48, 58, 22], 0.8189769358286867]]\n",
      "[[[56, 48, 58, 22, 46], 0.8244368232699344]]\n",
      "[[[56, 48, 58, 22, 46, 12], 0.9306152708741138]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84], 0.9374829655425856]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26], 1.0114391496317694]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82], 1.623242919697077]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36], 1.6363270342262695]]\n",
      "[56, 48, 58, 22, 46, 12, 84, 26, 82, 36]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 48, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "Sample 3\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'length', 'of', 'the', 'river', 'that', 'runs', 'through', 'the', 'most', 'states', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'distinct', 'river.length', 'from', 'river', 'where', 'river.river_name', '=', '(', 'select', 'river_name', 'from', '(', 'select', 'river.river_name', ',', 'count', '(', '1', ')', 'as', 'cnt', 'from', 'river', 'group', 'by', 'river.river_name', ')', 'tmp1', 'where', 'tmp1.cnt', '=', '(', 'select', 'max', '(', 'cnt', ')', 'from', '(', 'select', 'river.river_name', ',', 'count', '(', '1', ')', 'as', 'cnt', 'from', 'river', 'group', 'by', 'river.river_name', ')', 'tmp2', ')', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.length', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ')', ';']\n",
      "[[[56, 65], 0.5652518623101059]]\n",
      "[[[56, 65, 58], 0.5657236074330285]]\n",
      "[[[56, 65, 58, 22], 0.5662179328501225]]\n",
      "[[[56, 65, 58, 22, 46], 0.5819453466683626]]\n",
      "[[[56, 65, 58, 22, 46, 12], 0.6472832579165697]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84], 0.6506394916214049]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26], 0.6649939804337919]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82], 1.1881238012574613]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36], 1.1998331057839096]]\n",
      "[56, 65, 58, 22, 46, 12, 84, 26, 82, 36]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 82, 36, '<eos>'], inf]]\n",
      "Sample 4\n",
      "Source sentence = ['<sos>', 'which', 'state', 'has', 'the', 'most', 'rivers', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'river.traverse', 'from', 'river', 'group', 'by', 'river.traverse', 'order', 'by', 'count', '(', '1', ')', 'desc', 'limit', '1', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ')', ';']\n",
      "[[[56, 7], 0.07790302188368514]]\n",
      "[[[56, 7, 58], 0.0781158674362814]]\n",
      "[[[56, 7, 58, 69], 0.07823211388313212]]\n",
      "[[[56, 7, 58, 69, 46], 0.07867631281260401]]\n",
      "[[[56, 7, 58, 69, 46, 64], 0.09196434787008911]]\n",
      "[[[56, 7, 58, 69, 46, 64, 84], 0.13323265931103379]]\n",
      "[[[56, 7, 58, 69, 46, 64, 84, 5], 0.13468548422679305]]\n",
      "[[[56, 7, 58, 69, 46, 64, 84, 5, 36], 0.13489074891549535]]\n",
      "[56, 7, 58, 69, 46, 64, 84, 5, 36]\n",
      "[[[56, 7, 58, 69, 46, 64, 84, 5, 36, '<eos>'], inf]]\n",
      "Sample 5\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'highest', 'point', 'in', 'state@0', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'highlow.highest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'highlow.highest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';']\n",
      "[[[56, 65], 0.13500457655391074]]\n",
      "[[[56, 65, 58], 0.13503060929542698]]\n",
      "[[[56, 65, 58, 22], 0.13553570214025967]]\n",
      "[[[56, 65, 58, 22, 46], 0.13865123226605647]]\n",
      "[[[56, 65, 58, 22, 46, 12], 0.16651913806163066]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84], 0.18573606855534308]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26], 0.20296986124776595]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36], 0.21004800548143976]]\n",
      "[56, 65, 58, 22, 46, 12, 84, 26, 36]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "Sample 6\n",
      "Source sentence = ['<sos>', 'what', 'states', 'border', 'states', 'that', 'the', 'river@0', 'runs', 'through', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'border_info.border', 'from', 'border_info', 'where', 'border_info.state_name', 'in', '(', 'select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ';']\n",
      "[[[56, 65], 0.15769839603672153]]\n",
      "[[[56, 65, 58], 0.1579498946921376]]\n",
      "[[[56, 65, 58, 22], 0.1582681833206152]]\n",
      "[[[56, 65, 58, 22, 46], 0.16359637523055426]]\n",
      "[[[56, 65, 58, 22, 46, 12], 0.17559254666775814]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84], 0.176539308125939]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26], 0.1779123127598723]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36], 0.18092598809380434]]\n",
      "[56, 65, 58, 22, 46, 12, 84, 26, 36]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "[[[56, 65, 58, 22, 46, 12, 84, 26, 36, '<eos>'], inf]]\n",
      "Sample 7\n",
      "Source sentence = ['<sos>', 'which', 'states', 'border', 'the', 'river@0', 'river', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'border_info.border', 'from', 'border_info', 'where', 'border_info.state_name', 'in', '(', 'select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ';']\n",
      "[[[56, 78], 1.584213493306379]]\n",
      "[[[56, 78, 58], 1.5935738197604223]]\n",
      "[[[56, 78, 58, 33], 1.593767066968212]]\n",
      "[[[56, 78, 58, 33, 46], 1.6086048330344056]]\n",
      "[[[56, 78, 58, 33, 46, 40], 2.6942812646902894]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84], 2.6948251928715763]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9], 2.697621135464942]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56], 2.705084423027074]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34], 3.0987432100391743]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9], 3.0998327330034954]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40], 3.528063346473573]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82], 3.531216888297422]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58], 3.5570292273350788]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33], 3.5787308053859306]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82], 3.6074427930452657]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36], 3.676321019027455]]\n",
      "[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "[[[56, 78, 58, 33, 46, 40, 84, 9, 56, 34, 9, 40, 82, 58, 33, 82, 36, '<eos>'], inf]]\n",
      "Sample 8\n",
      "Source sentence = ['<sos>', 'which', 'city', 'in', 'state@0', 'has', 'the', 'largest', 'population', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'city.city_name', 'from', 'city', 'where', 'city.state_name', '=', 'state@0', 'and', 'city.population', '=', '(', 'select', 'max', '(', 'city.population', ')', 'from', 'city', 'where', 'city.state_name', '=', 'state@0', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'state.state_name', 'from', 'state', 'where', 'state.density', '=', '(', 'select', 'max', '(', 'state.density', ')', 'from', 'state', ')', ';']\n",
      "[[[56, 12], 0.7837141627169331]]\n",
      "[[[56, 12, 58], 0.7874825138787855]]\n",
      "[[[56, 12, 58, 22], 0.7934778154085507]]\n",
      "[[[56, 12, 58, 22, 46], 0.7941467494310928]]\n",
      "[[[56, 12, 58, 22, 46, 65], 0.8894151479544234]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84], 0.8909039821082843]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5], 0.8909670289503993]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36], 0.9119679208743037]]\n",
      "[56, 12, 58, 22, 46, 65, 84, 5, 36]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "[[[56, 12, 58, 22, 46, 65, 84, 5, 36, '<eos>'], inf]]\n",
      "Sample 9\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'length', 'of', 'the', 'longest', 'river', 'that', 'runs', 'through', 'state@0', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'river.length', 'from', 'river', 'where', 'river.traverse', '=', 'state@0', 'and', 'river.length', '=', '(', 'select', 'max', '(', 'river.length', ')', 'from', 'river', 'where', 'river.traverse', '=', 'state@0', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.river_name', 'from', 'river', 'where', 'river.traverse', '=', 'state@0', ';']\n"
     ]
    }
   ],
   "source": [
    "test_loss_beam = evaluate_withBeam(test_dataset, 10, sourceLanguage, targetLanguage, encoder, decoder, criterion, num_beam=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize models, optimizers, and a loss function (criterion).\n",
    "learning_rate = 0.0005  ##\n",
    "batch_size = 200  ##\n",
    "num_epochs = 100  ##\n",
    "clip_gradient = 5.0\n",
    "\n",
    "encoder_lstm_hidden_size = 100\n",
    "encoder_lstm_num_layers=1  #--\n",
    "encoder_dropout_prob = 0.05  ##\n",
    "\n",
    "decoder_target_embedding_dim=100\n",
    "decoder_lstm_num_layers=1  #--\n",
    "decoder_dropout_prob = 0.05  ##\n",
    "\n",
    "encoder = Encoder(source_language= sourceLanguage, \n",
    "                  lstm_hidden_size= encoder_lstm_hidden_size, \n",
    "                  lstm_num_layers= encoder_lstm_num_layers,\n",
    "                  dropout_prob= encoder_dropout_prob)\n",
    "decoder = AttnDecoder(target_language= targetLanguage, \n",
    "                      encoder= encoder, \n",
    "                      target_embedding_dim= decoder_target_embedding_dim, \n",
    "                      lstm_num_layers= decoder_lstm_num_layers,\n",
    "                      dropout_prob= decoder_dropout_prob)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epoch is 2.27607825993 dev loss= 1.15467170247\n",
      "Training loss after 1 epoch is 1.20679311445 dev loss= 1.20678237979\n",
      "Training loss after 2 epoch is 0.943579580331 dev loss= 1.19761471711\n",
      "Training loss after 3 epoch is 0.730745500779 dev loss= 1.71075533092\n",
      "Training loss after 4 epoch is 0.703775370299 dev loss= 1.79269359937\n",
      "Training loss after 5 epoch is 0.573016143514 dev loss= 1.86106495036\n",
      "Training loss after 6 epoch is 0.530724371274 dev loss= 1.33312898146\n",
      "Training loss after 7 epoch is 0.542422477901 dev loss= 0.746056127876\n",
      "Training loss after 8 epoch is 0.571043197164 dev loss= 1.68412802186\n",
      "Training loss after 9 epoch is 0.524221282079 dev loss= 1.53416181212\n",
      "Training loss after 10 epoch is 0.449801435331 dev loss= 0.694984872877\n",
      "Training loss after 11 epoch is 0.546943508527 dev loss= 1.36338791149\n",
      "Training loss after 12 epoch is 0.519769422816 dev loss= 2.20712772115\n",
      "Training loss after 13 epoch is 0.500848457615 dev loss= 1.34798843869\n",
      "Training loss after 14 epoch is 0.497981486866 dev loss= 2.1743650101\n",
      "Training loss after 15 epoch is 0.57005697173 dev loss= 1.66672793431\n",
      "Training loss after 16 epoch is 0.528098916369 dev loss= 0.56408076084\n",
      "Training loss after 17 epoch is 0.649519571151 dev loss= 1.35042806128\n",
      "Training loss after 18 epoch is 0.518462621651 dev loss= 1.55594670851\n",
      "Training loss after 19 epoch is 0.582457609127 dev loss= 1.458764254\n",
      "Training loss after 20 epoch is 0.58149921564 dev loss= 1.89068277412\n",
      "Training loss after 21 epoch is 0.505141977914 dev loss= 1.71835757749\n",
      "Training loss after 22 epoch is 0.471650133952 dev loss= 1.4618872596\n",
      "Training loss after 23 epoch is 0.507797846439 dev loss= 1.25078570028\n",
      "Training loss after 24 epoch is 0.465302468347 dev loss= 1.52592779331\n",
      "Training loss after 25 epoch is 0.501985092297 dev loss= 2.57949681182\n",
      "Training loss after 26 epoch is 0.523646191319 dev loss= 2.04451934381\n",
      "Training loss after 27 epoch is 0.42768757208 dev loss= 2.78269159531\n",
      "Training loss after 28 epoch is 0.494320074037 dev loss= 2.4484353415\n",
      "Training loss after 29 epoch is 0.483305144021 dev loss= 1.07488786355\n",
      "Training loss after 30 epoch is 0.529940388958 dev loss= 1.43010567873\n",
      "Training loss after 31 epoch is 0.436209876567 dev loss= 1.83981656331\n",
      "Training loss after 32 epoch is 0.498253981472 dev loss= 2.63160253961\n",
      "Training loss after 33 epoch is 0.496854550669 dev loss= 1.59314000796\n",
      "Training loss after 34 epoch is 0.486870707935 dev loss= 1.02042754397\n",
      "Training loss after 35 epoch is 0.500919465846 dev loss= 1.92894265537\n",
      "Training loss after 36 epoch is 0.477550157773 dev loss= 2.03113096195\n",
      "Training loss after 37 epoch is 0.485691929338 dev loss= 0.978652076493\n",
      "Training loss after 38 epoch is 0.493253012756 dev loss= 1.1295313487\n",
      "Training loss after 39 epoch is 0.561330463324 dev loss= 2.62681565542\n",
      "Training loss after 40 epoch is 0.586687620452 dev loss= 2.57529565462\n",
      "Training loss after 41 epoch is 0.436820276303 dev loss= 1.17792351037\n",
      "Training loss after 42 epoch is 0.454716409492 dev loss= 1.69110916732\n",
      "Training loss after 43 epoch is 0.472782298823 dev loss= 2.54250033484\n",
      "Training loss after 44 epoch is 0.482346699041 dev loss= 0.804183672011\n",
      "Training loss after 45 epoch is 0.437288631984 dev loss= 0.994602796327\n",
      "Training loss after 46 epoch is 0.510167096793 dev loss= 1.14251980761\n",
      "Training loss after 47 epoch is 0.472889336691 dev loss= 1.10847048517\n",
      "Training loss after 48 epoch is 0.473570401265 dev loss= 1.14466577044\n",
      "Training loss after 49 epoch is 0.489277999903 dev loss= 1.31390311649\n",
      "Training loss after 50 epoch is 0.500892410255 dev loss= 0.970180854511\n",
      "Training loss after 51 epoch is 0.526132212825 dev loss= 0.947464888491\n",
      "Training loss after 52 epoch is 0.580977027827 dev loss= 0.889805265869\n",
      "Training loss after 53 epoch is 0.500286751122 dev loss= 0.805716379061\n",
      "Training loss after 54 epoch is 0.471709135153 dev loss= 0.396257145906\n",
      "Training loss after 55 epoch is 0.470200469949 dev loss= 1.03994678011\n",
      "Training loss after 56 epoch is 0.490229570719 dev loss= 1.26954988453\n",
      "Training loss after 57 epoch is 0.413731411845 dev loss= 0.807965385234\n",
      "Training loss after 58 epoch is 0.408461987728 dev loss= 0.914877084917\n",
      "Training loss after 59 epoch is 0.53249873247 dev loss= 0.947607463848\n",
      "Training loss after 60 epoch is 0.425324065731 dev loss= 2.5669429477\n",
      "Training loss after 61 epoch is 0.537045940264 dev loss= 1.93051721801\n",
      "Training loss after 62 epoch is 0.401923790043 dev loss= 1.01313699893\n",
      "Training loss after 63 epoch is 0.453708503267 dev loss= 1.87935759996\n",
      "Training loss after 64 epoch is 0.435339191292 dev loss= 2.06793648999\n",
      "Training loss after 65 epoch is 0.429067455037 dev loss= 1.59784599055\n",
      "Training loss after 66 epoch is 0.521477632371 dev loss= 1.56862505666\n",
      "Training loss after 67 epoch is 0.438414932134 dev loss= 1.27949078651\n",
      "Training loss after 68 epoch is 0.426146461332 dev loss= 1.37635595671\n",
      "Training loss after 69 epoch is 0.426261803278 dev loss= 1.29162463077\n",
      "Training loss after 70 epoch is 0.366534236218 dev loss= 2.78226227839\n",
      "Training loss after 71 epoch is 0.346553516351 dev loss= 0.800939497193\n",
      "Training loss after 72 epoch is 0.433037638651 dev loss= 1.40736117598\n",
      "Training loss after 73 epoch is 0.393681089309 dev loss= 0.614686210304\n",
      "Training loss after 74 epoch is 0.404580054184 dev loss= 2.00784630989\n",
      "Training loss after 75 epoch is 0.407408076361 dev loss= 1.31043656371\n",
      "Training loss after 76 epoch is 0.422692364639 dev loss= 1.24524742819\n",
      "Training loss after 77 epoch is 0.355661178771 dev loss= 2.5647917131\n",
      "Training loss after 78 epoch is 0.523323270858 dev loss= 1.48377597998\n",
      "Training loss after 79 epoch is 0.361136651821 dev loss= 1.88940666697\n",
      "Training loss after 80 epoch is 0.459257508902 dev loss= 2.5180386502\n",
      "Training loss after 81 epoch is 0.462001814711 dev loss= 1.07200999872\n",
      "Training loss after 82 epoch is 0.42447810732 dev loss= 2.74562886745\n",
      "Training loss after 83 epoch is 0.51825814853 dev loss= 1.02271552445\n",
      "Training loss after 84 epoch is 0.528529218984 dev loss= 1.20689727884\n",
      "Training loss after 85 epoch is 0.358938773035 dev loss= 1.94141090826\n",
      "Training loss after 86 epoch is 0.50461417977 dev loss= 0.965235757184\n",
      "Training loss after 87 epoch is 0.449831256416 dev loss= 1.42871474694\n",
      "Training loss after 88 epoch is 0.570771953249 dev loss= 1.67024018163\n",
      "Training loss after 89 epoch is 0.551149236711 dev loss= 0.488221273425\n",
      "Training loss after 90 epoch is 0.369304916316 dev loss= 1.54890791264\n",
      "Training loss after 91 epoch is 0.459886407239 dev loss= 1.2654616669\n",
      "Training loss after 92 epoch is 0.471830634847 dev loss= 0.791744852759\n",
      "Training loss after 93 epoch is 0.457996381529 dev loss= 1.58646278561\n",
      "Training loss after 94 epoch is 0.368853570688 dev loss= 0.572309120906\n",
      "Training loss after 95 epoch is 0.392191115421 dev loss= 1.93982417229\n",
      "Training loss after 96 epoch is 0.460544270035 dev loss= 1.91795829977\n",
      "Training loss after 97 epoch is 0.548175819718 dev loss= 1.43793569238\n",
      "Training loss after 98 epoch is 0.453327164278 dev loss= 0.525609575049\n",
      "Training loss after 99 epoch is 0.457138663217 dev loss= 1.25623387195\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for n_epochs in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    for batch in range(batch_size):\n",
    "        sample = random.choice(training_dataset)\n",
    "        source_language_sentence, target_language_sentence = Variable(sample['nl']), Variable(sample['sql'])\n",
    "        training_loss += train(source_language_sentence, target_language_sentence, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, clip_gradient)\n",
    "    \n",
    "    training_loss = training_loss/batch_size\n",
    "    dev_loss = evaluate(dev_dataset, 10, sourceLanguage, targetLanguage, encoder, decoder, criterion)\n",
    "    print 'Training loss after ' + str(n_epochs) + ' epoch is ' + str(training_loss) + ' dev loss= ' + str(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 0\n",
      "Source sentence = ['<sos>', 'what', 'states', 'have', 'cities', 'named', 'city@0', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'city.state_name', 'from', 'city', 'where', 'city.city_name', '=', 'city@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'city.state_name', 'from', 'city', 'where', 'city.city_name', '=', 'city@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 1\n",
      "Source sentence = ['<sos>', 'which', 'state', 'has', 'the', 'highest', 'elevation', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'highlow.state_name', 'from', 'highlow', 'where', 'highlow.highest_elevation', '=', '(', 'select', 'max', '(', 'highlow.highest_elevation', ')', 'from', 'highlow', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'state.state_name', 'from', 'state', 'where', 'state.density', '=', '(', 'select', 'max', '(', 'highlow.highest_elevation', ')', 'from', 'highlow', ')', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 2\n",
      "Source sentence = ['<sos>', 'which', 'states', 'border', 'state@0', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'border_info.border', 'from', 'border_info', 'where', 'border_info.state_name', '=', 'state@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'border_info.border', 'from', 'border_info', 'where', 'border_info.state_name', '=', 'state@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 3\n",
      "Source sentence = ['<sos>', 'where', 'is', 'the', 'highest', 'point', 'in', 'state@0', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'highlow.highest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'highlow.lowest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 4\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'highest', 'point', 'in', 'state@0', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'highlow.highest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'highlow.lowest_point', 'from', 'highlow', 'where', 'highlow.state_name', '=', 'state@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 5\n",
      "Source sentence = ['<sos>', 'which', 'river', 'runs', 'through', 'most', 'states', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'river.river_name', 'from', 'river', 'group', 'by', '(', 'river.river_name', ')', 'order', 'by', 'count', '(', 'distinct', 'river.traverse', ')', 'desc', 'limit', '1', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.river_name', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 6\n",
      "Source sentence = ['<sos>', 'which', 'state', 'is', 'mount', 'mountain@0', 'in', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'mountain.state_name', 'from', 'mountain', 'where', 'mountain.mountain_name', '=', 'mountain@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'city.state_name', 'from', 'city', 'where', 'city.city_name', '=', 'city@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 7\n",
      "Source sentence = ['<sos>', 'how', 'many', 'states', 'does', 'state@0', 'border', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'count', '(', 'border_info.border', ')', 'from', 'border_info', 'where', 'border_info.state_name', '=', 'state@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'count', '(', 'border_info.border', ')', 'from', 'border_info', 'where', 'border_info.state_name', '=', 'state@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 8\n",
      "Source sentence = ['<sos>', 'which', 'states', 'does', 'the', 'river@0', 'run', 'through', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'river.traverse', 'from', 'river', 'where', 'river.river_name', '=', 'river@0', ';']\n",
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "Sample 9\n",
      "Source sentence = ['<sos>', 'what', 'is', 'the', 'most', '<unk>', 'state', 'in', 'the', 'united', 'states', '<eos>']\n",
      "Target sentence = ['<sos>', 'select', 'state.state_name', 'from', 'state', 'where', 'state.density', '=', '(', 'select', 'max', '(', 'state.density', ')', 'from', 'state', ')', ';', '<eos>']\n",
      "Predicted sentence = ['select', 'sum', '(', 'state.area', ')', 'from', 'state', ';']\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on small sample of test set\n",
    "test_loss = evaluate(test_dataset, 10, sourceLanguage, targetLanguage, encoder, decoder, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1233193239597012"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Incomplete Beam search-- fill it up or throw it and implement from scratch\n",
    "class State(object):\n",
    "    def __init__(self, decoder_input, decoder_hidden_state, decoder_cell_state, encoder_outputs, current_score, decoded_sentence):\n",
    "            self.decoder_input = decoder_input\n",
    "            self.decoder_hidden_state = decoder_hidden_state\n",
    "            self.decoder_cell_state = decoder_cell_state\n",
    "            self.encoder_outputs = encoder_outputs\n",
    "            self.current_score = current_score\n",
    "            self.decoded_sentence = decoded_sentence\n",
    "\n",
    "class BeamSearch(object):\n",
    "    def __init__(self, beam_size, decoder):\n",
    "        self.beam_size = beam_size\n",
    "        self.decoder = decoder\n",
    "        self.current_states = []\n",
    "        self.next_states = []\n",
    "    \n",
    "    # initialize first state in current state\n",
    "    def initialize_beam_search(self, decoder_input, decoder_hidden_state, decoder_cell_state, encoder_outputs):\n",
    "    \n",
    "    #performs one step of beam search\n",
    "    def forward(self):\n",
    "        for state in self.current_states:\n",
    "            decoder_output, decoder_hidden_state, decoder_cell_state = self.decoder(state.decoder_input, \n",
    "                                                                                    state.decoder_hidden_state, \n",
    "                                                                                    state.decoder_cell_state,\n",
    "                                                                                    state.encoder_outputs)\n",
    "            self.next_states.append(self.get_next_states(decoder_output, decoder_hidden_state, decoder_cell_state, state.encoder_outputs, state.current_score, state.decoded_sentence))\n",
    "        \n",
    "        self.current_states[:] = get_best_states(self.beam_size, self.next_states)\n",
    "        self.next_states[:] = []\n",
    "        \n",
    "    #computes the next states\n",
    "    def get_next_states():\n",
    "    \n",
    "    #sorts the states based on score\n",
    "    def get_best_states():\n",
    "    \n",
    "    def search(self):\n",
    "        self.initialize_beam_search()\n",
    "        # call forward in loop to perform one step of beam search \n",
    "        # do untill EOS is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 0.025600863289563108]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 0.03384250043584397]\n",
      "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 0.03384250043584397]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# beam search\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 1.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score * -log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences\n",
    "\n",
    "# define a sequence of 10 words over a vocab of 5 words\n",
    "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "\t\t[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t[0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "data = array(data)\n",
    "# decode sequence\n",
    "result = beam_search_decoder(data, 3)\n",
    "# print result\n",
    "for seq in result:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "\n",
      "Columns 0 to 7 \n",
      "  1.0000   2.0000   3.0000   4.0000   5.0000   6.0000   7.0000   8.9000\n",
      "\n",
      "Columns 8 to 10 \n",
      " 10.0000  11.0000  12.0000\n",
      "[torch.FloatTensor of size 1x11]\n",
      "]\n",
      "\n",
      "\n",
      "Columns 0 to 7 \n",
      "  1.0000   2.0000   3.0000   4.0000   5.0000   6.0000   7.0000   8.9000\n",
      "\n",
      "Columns 8 to 10 \n",
      " 10.0000  11.0000  12.0000\n",
      "[torch.FloatTensor of size 1x11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([[1,2,3,4,5,6,7,8.9,10,11,12]])\n",
    "b = []\n",
    "b.append(a)\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[[1,2,3],-1], [[1,2,4],-2], [[2,2,3],-3], [[0,2,3],1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2, 2, 3], -3], [[1, 2, 4], -2], [[1, 2, 3], -1]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = sorted(a, key=lambda lst: lst[1])[:3]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('inf') >1e100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
